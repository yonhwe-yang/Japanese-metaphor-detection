{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe571d5-b9a3-4579-9694-26a6b34bc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Âä†ÈÄü\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e8df34-620a-4a42-9021-5ff2178c098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÁºìÂ≠òÊï∞ÊçÆÁõò\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_home\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/root/autodl-tmp/hf_datasets\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "os.environ[\"TRANSFORMERS_NO_MLX\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3888bda-c0de-481d-9b4e-0b06b74053d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cv.py(K-FOLD)\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "\n",
    "#ÈÖçÁΩÆ\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"ku-nlp/deberta-v3-base-japanese\"\n",
    "    data_files: list = None  \n",
    "    label2id: dict = None\n",
    "    n_splits: int = 5\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_train_epochs: int = 30\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    dropout_prob: float = 0.4\n",
    "    class_weights: list = None  # [1.0, 3.0] for [\"O\", \"MET\"]\n",
    "    eval_steps: int = 500\n",
    "    early_stopping_patience: int = 20\n",
    "    fp16: bool = True\n",
    "    output_dir_base: str = \"./autodl-tmp/tlv_sqe\"\n",
    "    save_total_limit: int = 1 \n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data_files is None:\n",
    "            self.data_files = [\n",
    "                (\"data_ketsugou.json\", \"metaphor\"),\n",
    "                (\"data_shihyou.json\", \"simile\"),\n",
    "                (\"data_no.json\", \"non_metaphor\"),\n",
    "            ]\n",
    "        if self.label2id is None:\n",
    "            self.label2id = {\"O\": 0, \"MET\": 1}\n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = [1.0, 3.0]\n",
    "\n",
    "\n",
    "#Âä†ÊùÉÊçüÂ§± Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        device = outputs.logits.device\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        num_labels = outputs.logits.size(-1)\n",
    "        loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Ê∏ÖÁêÜcheckpoint\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    output_path = Path(output_dir)\n",
    "    for ckpt_dir in output_path.glob(\"checkpoint-*\"):\n",
    "        print(f\"üóëÔ∏è Âà†Èô§‰∏≠Èó¥ checkpoint: {ckpt_dir}\")\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "def load_data_with_tag(filename, tag):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        d[\"source_tag\"] = tag\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tokenize + ÂØπÈΩêÊ†áÁ≠æ\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256, \n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[labels[wid]])\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# ËØÑ‰ª∑\n",
    "\n",
    "def build_compute_metrics(id2label):\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_preds = [\n",
    "            [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "            for pred, label in zip(predictions, labels)\n",
    "        ]\n",
    "        res = metric.compute(predictions=true_preds, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": res[\"overall_precision\"],\n",
    "            \"recall\": res[\"overall_recall\"],\n",
    "            \"f1\": res[\"overall_f1\"],\n",
    "            \"accuracy\": res[\"overall_accuracy\"]\n",
    "        }\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "# ÁßçÂ≠ê\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# ------------main----------------\n",
    "def main():\n",
    "    config = Config()\n",
    "    set_seed(config.random_state)\n",
    "\n",
    "    print(\"üöÄ CUDA Available:\", torch.cuda.is_available())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---------- 1. Âä†ËΩΩÊï∞ÊçÆ ----------\n",
    "    print(\"üì• ËØªÂèñËØ≠Êñô...\")\n",
    "    all_data = []\n",
    "    for filename, tag in config.data_files:\n",
    "        data = load_data_with_tag(filename, tag)\n",
    "        all_data.extend(data)\n",
    "    labels_for_split = [d[\"source_tag\"] for d in all_data]\n",
    "    print(f\"üìä ÊÄªÊ†∑Êú¨Êï∞: {len(all_data)}\")\n",
    "\n",
    "    # ---------- 2. ÂàùÂßãÂåñÂàÜËØçÂô® & Ê†áÁ≠æÊò†Â∞Ñ ----------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    label2id = config.label2id\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    # ---------- 3. ÊûÑÂª∫ tokenize ÂáΩÊï∞ ----------\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_and_align_labels(examples, tokenizer, label2id)\n",
    "\n",
    "    # ---------- 4. ÊûÑÂª∫ËØÑ‰ª∑ÂáΩÊï∞ ----------\n",
    "    compute_metrics_fn = build_compute_metrics(id2label)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # ---------- 5. K Êäò‰∫§ÂèâÈ™åËØÅ ----------\n",
    "    kf = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.random_state)\n",
    "    all_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_data, labels_for_split)):\n",
    "        print(f\"\\n{'='*20} Fold {fold+1} {'='*20}\")\n",
    "\n",
    "        # ÂàõÂª∫Â∏¶Êó∂Èó¥Êà≥ÁöÑËæìÂá∫ÁõÆÂΩï\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"{config.output_dir_base}{fold+1}_{timestamp}\"\n",
    "\n",
    "        train_data = [all_data[i] for i in train_idx]\n",
    "        val_data = [all_data[i] for i in val_idx]\n",
    "\n",
    "        train_ds = Dataset.from_list(train_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "        val_ds = Dataset.from_list(val_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            config.model_name,\n",
    "            num_labels=len(label2id),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            hidden_dropout_prob=config.dropout_prob,\n",
    "        )\n",
    "        # ‚ùó ‰∏çË¶ÅÊâãÂä® .to(device) ‚Äî Trainer ‰ºöËá™Âä®Â§ÑÁêÜ\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.batch_size,\n",
    "            per_device_eval_batch_size=config.batch_size // 2,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=config.eval_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=config.eval_steps,\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            fp16=config.fp16 and torch.cuda.is_available(),\n",
    "            dataloader_num_workers=2,\n",
    "            report_to=\"none\",\n",
    "            save_total_limit=config.save_total_limit,  # Âè™‰øùÁïô best\n",
    "            seed=config.random_state,\n",
    "        )\n",
    "\n",
    "        trainer = WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics_fn,\n",
    "            class_weights=config.class_weights,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)],\n",
    "        )\n",
    "\n",
    "        print(\"‚è≥ ÂºÄÂßãËÆ≠ÁªÉ...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # üßπ Ê∏ÖÁêÜ‰∏≠Èó¥ checkpointÔºåÂè™‰øùÁïô best\n",
    "        cleanup_checkpoints(output_dir)\n",
    "\n",
    "        # üíæ ‰øùÂ≠òÊúÄÁªàÊ®°ÂûãÂíåÂàÜËØçÂô®\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # üìà ËØÑ‰º∞\n",
    "        eval_res = trainer.evaluate()\n",
    "        print(\"‚úÖ ËØÑ‰º∞ÁªìÊûú:\", eval_res)\n",
    "        all_results.append(eval_res)\n",
    "\n",
    "        # üìÇ ‰øùÂ≠òÊú¨ÊäòÁªìÊûú\n",
    "        with open(f\"{output_dir}/eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(eval_res, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # ---------- 6. ËÆ°ÁÆóÂπ≥ÂùáÁªìÊûú ----------\n",
    "    avg_precision = np.mean([r[\"eval_precision\"] for r in all_results])\n",
    "    avg_recall = np.mean([r[\"eval_recall\"] for r in all_results])\n",
    "    avg_f1 = np.mean([r[\"eval_f1\"] for r in all_results])\n",
    "\n",
    "    final_results = {\n",
    "        \"avg_precision\": float(avg_precision),\n",
    "        \"avg_recall\": float(avg_recall),\n",
    "        \"avg_f1\": float(avg_f1),\n",
    "        \"fold_results\": all_results\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"üèÜ ÊúÄÁªàÂπ≥ÂùáÁªìÊûú\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"F1:        {avg_f1:.4f}\")\n",
    "\n",
    "    # üíæ ‰øùÂ≠òÊúÄÁªàÁªìÊûú\n",
    "    with open(\"final_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nüíæ ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: final_results.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdbacb3-7a4e-4f35-a551-594857923a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CUDA Available: True\n",
      "üì• ËØªÂèñËØ≠Êñô...\n",
      "üìä ÊÄªÊ†∑Êú¨Êï∞: 9185\n",
      "üßÆ ËÆ≠ÁªÉÈõÜ: 7348 | È™åËØÅÈõÜ: 1837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7348/7348 [00:01<00:00, 4523.12 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1837/1837 [00:00<00:00, 2670.22 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ku-nlp/deberta-v3-base-japanese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_10116/2350339127.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 96868, 'bos_token_id': 96871, 'pad_token_id': 96869}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ÂºÄÂßãËÆ≠ÁªÉ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1740/1740 13:14, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.099482</td>\n",
       "      <td>0.799458</td>\n",
       "      <td>0.917574</td>\n",
       "      <td>0.854453</td>\n",
       "      <td>0.967379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.176725</td>\n",
       "      <td>0.869056</td>\n",
       "      <td>0.923795</td>\n",
       "      <td>0.895590</td>\n",
       "      <td>0.976272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.149596</td>\n",
       "      <td>0.863112</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.896036</td>\n",
       "      <td>0.976078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Âà†Èô§‰∏≠Èó¥ checkpoint: autodl-tmp/tlv_sqe/result_single_20250917_210317/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ÊúÄÁªàËØÑ‰º∞ÁªìÊûú:\n",
      "  eval_loss: 0.1496\n",
      "  eval_precision: 0.8631\n",
      "  eval_recall: 0.9316\n",
      "  eval_f1: 0.8960\n",
      "  eval_accuracy: 0.9761\n",
      "  eval_runtime: 5.4234\n",
      "  eval_samples_per_second: 338.7150\n",
      "  eval_steps_per_second: 42.4090\n",
      "  epoch: 30.0000\n",
      "\n",
      "üéâ Ê®°Âûã‰∏éÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: ./autodl-tmp/tlv_sqe/result_single_20250917_210317\n"
     ]
    }
   ],
   "source": [
    "#Êó†k-fold\n",
    "# train_single.py\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split  # üëà ÊîπÁî®ÁÆÄÂçïÂàíÂàÜ\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# ----------------------------\n",
    "# ‚öôÔ∏è ÈÖçÁΩÆÁ±ª\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"ku-nlp/deberta-v3-base-japanese\"\n",
    "    data_files: list = None\n",
    "    label2id: dict = None\n",
    "    random_state: int = 42\n",
    "    train_ratio: float = 0.8  # ËÆ≠ÁªÉÈõÜÊØî‰æã\n",
    "    batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_train_epochs: int = 30  #\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    dropout_prob: float = 0.4\n",
    "    class_weights: list = None\n",
    "    eval_steps: int = 500\n",
    "    early_stopping_patience: int = 20\n",
    "    fp16: bool = True\n",
    "    output_dir: str =  \"./autodl-tmp/tlv_sqe/result_single\"\n",
    "    save_total_limit: int = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data_files is None:\n",
    "            self.data_files = [\n",
    "                (\"data_ketsugou.json\", \"metaphor\"),\n",
    "                (\"data_shihyou.json\", \"simile\"),\n",
    "                (\"data_no.json\", \"non_metaphor\"),\n",
    "            ]\n",
    "        if self.label2id is None:\n",
    "            self.label2id = {\"O\": 0, \"MET\": 1}\n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = [1.0, 30.0]\n",
    "\n",
    "# ----------------------------\n",
    "# üß† Âä†ÊùÉÊçüÂ§± Trainer\n",
    "# ----------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        device = outputs.logits.device\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        num_labels = outputs.logits.size(-1)\n",
    "        loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ----------------------------\n",
    "# üßπ Ê∏ÖÁêÜ‰∏≠Èó¥ checkpoint\n",
    "# ----------------------------\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    output_path = Path(output_dir)\n",
    "    for ckpt_dir in output_path.glob(\"checkpoint-*\"):\n",
    "        print(f\"üóëÔ∏è Âà†Èô§‰∏≠Èó¥ checkpoint: {ckpt_dir}\")\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "# ----------------------------\n",
    "# üì• Âä†ËΩΩÊï∞ÊçÆÂáΩÊï∞\n",
    "# ----------------------------\n",
    "def load_data_with_tag(filename, tag):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        d[\"source_tag\"] = tag\n",
    "    return data\n",
    "\n",
    "# ----------------------------\n",
    "# üß© Tokenize + ÂØπÈΩêÊ†áÁ≠æ\n",
    "# ----------------------------\n",
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[labels[wid]])\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "# ----------------------------\n",
    "# üìä ËØÑ‰ª∑ÊåáÊ†á\n",
    "# ----------------------------\n",
    "def build_compute_metrics(id2label):\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_preds = [\n",
    "            [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "            for pred, label in zip(predictions, labels)\n",
    "        ]\n",
    "        res = metric.compute(predictions=true_preds, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": res[\"overall_precision\"],\n",
    "            \"recall\": res[\"overall_recall\"],\n",
    "            \"f1\": res[\"overall_f1\"],\n",
    "            \"accuracy\": res[\"overall_accuracy\"]\n",
    "        }\n",
    "    return compute_metrics\n",
    "\n",
    "# ----------------------------\n",
    "# üå± ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "# ----------------------------\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----------------------------\n",
    "# üöÄ ‰∏ªÁ®ãÂ∫èÔºàÂçïÊ¨°ËÆ≠ÁªÉÔºâ\n",
    "# ----------------------------\n",
    "def main():\n",
    "    config = Config()\n",
    "    set_seed(config.random_state)\n",
    "\n",
    "    print(\"üöÄ CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "    # ---------- 1. Âä†ËΩΩÊï∞ÊçÆ ----------\n",
    "    print(\"üì• ËØªÂèñËØ≠Êñô...\")\n",
    "    all_data = []\n",
    "    for filename, tag in config.data_files:\n",
    "        data = load_data_with_tag(filename, tag)\n",
    "        all_data.extend(data)\n",
    "    print(f\"üìä ÊÄªÊ†∑Êú¨Êï∞: {len(all_data)}\")\n",
    "\n",
    "    # ---------- 2. ÂàíÂàÜËÆ≠ÁªÉÈõÜ/È™åËØÅÈõÜ ----------\n",
    "    train_data, val_data = train_test_split(\n",
    "        all_data,\n",
    "        train_size=config.train_ratio,\n",
    "        random_state=config.random_state,\n",
    "        shuffle=True,\n",
    "        stratify=[d[\"source_tag\"] for d in all_data]  # ÂàÜÂ±ÇÊäΩÊ†∑Ôºå‰øùÊåÅÊØî‰æã\n",
    "    )\n",
    "    print(f\"üßÆ ËÆ≠ÁªÉÈõÜ: {len(train_data)} | È™åËØÅÈõÜ: {len(val_data)}\")\n",
    "\n",
    "    # ---------- 3. ÂàùÂßãÂåñÂàÜËØçÂô® & Ê†áÁ≠æÊò†Â∞Ñ ----------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    label2id = config.label2id\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    # ---------- 4. ÊûÑÂª∫ tokenize ÂáΩÊï∞ ----------\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_and_align_labels(examples, tokenizer, label2id)\n",
    "\n",
    "    # ---------- 5. ÊûÑÂª∫ËØÑ‰ª∑ÂáΩÊï∞ ----------\n",
    "    compute_metrics_fn = build_compute_metrics(id2label)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # ÂàõÂª∫Â∏¶Êó∂Èó¥Êà≥ÁöÑËæìÂá∫ÁõÆÂΩï\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"{config.output_dir}_{timestamp}\"\n",
    "\n",
    "    train_ds = Dataset.from_list(train_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "    val_ds = Dataset.from_list(val_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        hidden_dropout_prob=config.dropout_prob,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size // 2,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        weight_decay=config.weight_decay,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config.eval_steps,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=config.fp16 and torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        seed=config.random_state,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        class_weights=config.class_weights,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)],\n",
    "    )\n",
    "\n",
    "    print(\"‚è≥ ÂºÄÂßãËÆ≠ÁªÉ...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # üßπ Ê∏ÖÁêÜ‰∏≠Èó¥ checkpoint\n",
    "    cleanup_checkpoints(output_dir)\n",
    "\n",
    "    # üíæ ‰øùÂ≠òÊúÄÁªàÊ®°Âûã\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # üìà ÊúÄÁªàËØÑ‰º∞\n",
    "    eval_res = trainer.evaluate()\n",
    "    print(\"\\n‚úÖ ÊúÄÁªàËØÑ‰º∞ÁªìÊûú:\")\n",
    "    for k, v in eval_res.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    # üìÇ ‰øùÂ≠òÁªìÊûú\n",
    "    with open(f\"{output_dir}/eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_res, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nüéâ Ê®°Âûã‰∏éÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65b1fa58-1490-4523-b8c8-43ff23edc8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È¢ÑÊµãÁªìÊûúÔºö\n",
      "[CLS]\tO\n",
      "‰Ωï\tO\n",
      "„Åã„Åå\tO\n",
      "ÁáÉ„Åà\tMET\n",
      "„Çã\tMET\n",
      "[SEP]\tMET\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# ======================\n",
    "# 0. ËÆæÁΩÆËÆæÂ§á\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================\n",
    "# 1. Âä†ËΩΩÊ®°ÂûãÂíå tokenizer\n",
    "# ======================\n",
    "model_name_or_path = \"./autodl-tmp/tlv_sqe/result_single_20250917_210317\"  # ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãË∑ØÂæÑ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# label Êò†Â∞Ñ\n",
    "id2label = {0: \"O\", 1: \"MET\"}\n",
    "\n",
    "# ======================\n",
    "# 2. È¢ÑÊµãÂáΩÊï∞\n",
    "# ======================\n",
    "def predict_raw_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    ËæìÂÖ•ÂéüÂßãÂè•Â≠êÔºàstrÔºâÔºåËøîÂõû List[(token, label)]\n",
    "    \"\"\"\n",
    "    # ‰ΩøÁî® tokenizer Ëá™Âä®ÂàÜËØç\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False  \n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    # ÂØπÈΩê subword token\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    result = list(zip(tokens, [id2label[p] for p in predictions]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ÊµãËØï\n",
    "\n",
    "sentence = \"‰Ωï„Åã„ÅåÁáÉ„Åà„Çã\"\n",
    "result = predict_raw_sentence(sentence)\n",
    "\n",
    "print(\"È¢ÑÊµãÁªìÊûúÔºö\")\n",
    "for token, label in result:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bf6971-c13d-4c69-ac61-2a0eeef62d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 sentences...\n",
      "Processed 200 sentences...\n",
      "Processed 300 sentences...\n",
      "Processed 400 sentences...\n",
      "Processed 500 sentences...\n",
      "Processed 600 sentences...\n",
      "Processed 700 sentences...\n",
      "Processed 800 sentences...\n",
      "Processed 900 sentences...\n",
      "Processed 1000 sentences...\n",
      "Processed 1100 sentences...\n",
      "Processed 1200 sentences...\n",
      "Processed 1300 sentences...\n",
      "Processed 1400 sentences...\n",
      "Processed 1500 sentences...\n",
      "Processed 1600 sentences...\n",
      "Processed 1700 sentences...\n",
      "Processed 1800 sentences...\n",
      "Processed 1900 sentences...\n",
      "Processed 2000 sentences...\n",
      "Processed 2100 sentences...\n",
      "Processed 2200 sentences...\n",
      "Processed 2300 sentences...\n",
      "Processed 2400 sentences...\n",
      "Processed 2500 sentences...\n",
      "Processed 2600 sentences...\n",
      "Processed 2700 sentences...\n",
      "Processed 2800 sentences...\n",
      "Processed 2900 sentences...\n",
      "Processed 3000 sentences...\n",
      "Processed 3100 sentences...\n",
      "Processed 3200 sentences...\n",
      "Processed 3300 sentences...\n",
      "Processed 3400 sentences...\n",
      "Processed 3500 sentences...\n",
      "Processed 3600 sentences...\n",
      "Processed 3700 sentences...\n",
      "Processed 3800 sentences...\n",
      "Processed 3900 sentences...\n",
      "Processed 4000 sentences...\n",
      "Processed 4100 sentences...\n",
      "Processed 4200 sentences...\n",
      "Processed 4300 sentences...\n",
      "Processed 4400 sentences...\n",
      "Processed 4500 sentences...\n",
      "Processed 4600 sentences...\n",
      "Processed 4700 sentences...\n",
      "Processed 4800 sentences...\n",
      "Processed 4900 sentences...\n",
      "Processed 5000 sentences...\n",
      "Processed 5100 sentences...\n",
      "‚úÖ Processing complete. Results saved to met_tokens_output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# ======================\n",
    "# 0. ËÆæÁΩÆËÆæÂ§á\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================\n",
    "# 1. Âä†ËΩΩÊ®°ÂûãÂíå tokenizer\n",
    "# ======================\n",
    "model_name_or_path = \"./autodl-tmp/tlv_sqe/result_single_20250917_210317\"  # ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãË∑ØÂæÑ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# label Êò†Â∞Ñ\n",
    "id2label = {0: \"O\", 1: \"MET\"}\n",
    "\n",
    "# ======================\n",
    "# 2. È¢ÑÊµãÂáΩÊï∞ÔºàËøîÂõû token + label ÂàóË°®Ôºâ\n",
    "# ======================\n",
    "def predict_raw_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    ËæìÂÖ•ÂéüÂßãÂè•Â≠êÔºàstrÔºâÔºåËøîÂõû List[(token, label)]\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    result = list(zip(tokens, [id2label.get(p, \"O\") for p in predictions]))\n",
    "    return result\n",
    "\n",
    "# ======================\n",
    "# 3. ÊâπÈáèÂ§ÑÁêÜÂáΩÊï∞\n",
    "# ======================\n",
    "def process_txt_file(input_txt_path: str, output_csv_path: str):\n",
    "    \"\"\"\n",
    "    ËØªÂèñ txt Êñá‰ª∂ÔºàÊØèË°å‰∏ÄÂè•ÔºâÔºåÈ¢ÑÊµãÊØèÂè•‰∏≠ÁöÑ MET tokenÔºå\n",
    "    Âπ∂Â∞ÜÁªìÊûúÂÜôÂÖ• CSV Êñá‰ª∂„ÄÇ\n",
    "    \"\"\"\n",
    "    with open(input_txt_path, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_csv_path, 'w', newline='', encoding='utf-8') as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "        # ÂÜôÂÖ•Ë°®Â§¥\n",
    "        writer.writerow([\"sentence\", \"token\", \"position_in_sentence\"])\n",
    "\n",
    "        line_num = 0\n",
    "        for line in f_in:\n",
    "            line_num += 1\n",
    "            sentence = line.strip()\n",
    "            if not sentence:\n",
    "                continue  # Ë∑≥ËøáÁ©∫Ë°å\n",
    "\n",
    "            try:\n",
    "                predictions = predict_raw_sentence(sentence)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_num}: {sentence} | {e}\")\n",
    "                continue\n",
    "\n",
    "            # ÈÅçÂéÜÈ¢ÑÊµãÁªìÊûúÔºåÊâæÂá∫ MET Ê†áÁ≠æÁöÑ token\n",
    "            for idx, (token, label) in enumerate(predictions):\n",
    "                if label == \"MET\":\n",
    "                    # ËøáÊª§ÊéâÁâπÊÆä token Â¶Ç [CLS], [SEP], [PAD] Á≠â\n",
    "                    if token in tokenizer.all_special_tokens:\n",
    "                        continue\n",
    "                    # ÂèØÈÄâÔºöËøòÂéü ## ÂºÄÂ§¥ÁöÑ subwordÔºàÂ¶Ç \"##ing\"Ôºâ\n",
    "                    if token.startswith(\"##\") and idx > 0:\n",
    "                        # ÂèØ‰ª•ÈÄâÊã©ÂêàÂπ∂Ôºå‰ΩÜ‰∏∫‰∫ÜÁÆÄÂçïÔºåËøôÈáåÂè™ËÆ∞ÂΩïÂéüÂßã token\n",
    "                        pass\n",
    "                    writer.writerow([sentence, token, idx])\n",
    "\n",
    "            # ÂèØÈÄâÔºöÊâìÂç∞ËøõÂ∫¶\n",
    "            if line_num % 100 == 0:\n",
    "                print(f\"Processed {line_num} sentences...\")\n",
    "\n",
    "    print(f\"‚úÖ Processing complete. Results saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4. ‰∏ªÁ®ãÂ∫èÂÖ•Âè£\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_TXT = \"abe_speech_5000.txt\"   # ÊõøÊç¢‰∏∫‰Ω†ÁöÑËæìÂÖ•Êñá‰ª∂Ë∑ØÂæÑ\n",
    "    OUTPUT_CSV = \"met_tokens_output.csv\"\n",
    "\n",
    "    process_txt_file(INPUT_TXT, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c37c0-ae5f-4256-8a7a-a22ee30d6ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elyza",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
