{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe571d5-b9a3-4579-9694-26a6b34bc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e8df34-620a-4a42-9021-5ff2178c098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缓存数据盘\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/root/autodl-tmp/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/hf_home\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/root/autodl-tmp/hf_datasets\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "os.environ[\"TRANSFORMERS_NO_MLX\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3888bda-c0de-481d-9b4e-0b06b74053d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cv.py(K-FOLD)\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "\n",
    "#配置\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"ku-nlp/deberta-v3-base-japanese\"\n",
    "    data_files: list = None  \n",
    "    label2id: dict = None\n",
    "    n_splits: int = 5\n",
    "    random_state: int = 42\n",
    "    batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_train_epochs: int = 30\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    dropout_prob: float = 0.4\n",
    "    class_weights: list = None  # [1.0, 3.0] for [\"O\", \"MET\"]\n",
    "    eval_steps: int = 500\n",
    "    early_stopping_patience: int = 20\n",
    "    fp16: bool = True\n",
    "    output_dir_base: str = \"./autodl-tmp/tlv_sqe\"\n",
    "    save_total_limit: int = 1 \n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data_files is None:\n",
    "            self.data_files = [\n",
    "                (\"data_ketsugou.json\", \"metaphor\"),\n",
    "                (\"data_shihyou.json\", \"simile\"),\n",
    "                (\"data_no.json\", \"non_metaphor\"),\n",
    "            ]\n",
    "        if self.label2id is None:\n",
    "            self.label2id = {\"O\": 0, \"MET\": 1}\n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = [1.0, 3.0]\n",
    "\n",
    "\n",
    "#加权损失 Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        device = outputs.logits.device\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        num_labels = outputs.logits.size(-1)\n",
    "        loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# 清理checkpoint\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    output_path = Path(output_dir)\n",
    "    for ckpt_dir in output_path.glob(\"checkpoint-*\"):\n",
    "        print(f\"🗑️ 删除中间 checkpoint: {ckpt_dir}\")\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "def load_data_with_tag(filename, tag):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        d[\"source_tag\"] = tag\n",
    "    return data\n",
    "\n",
    "\n",
    "# Tokenize + 对齐标签\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256, \n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[labels[wid]])\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# 评价\n",
    "\n",
    "def build_compute_metrics(id2label):\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_preds = [\n",
    "            [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "            for pred, label in zip(predictions, labels)\n",
    "        ]\n",
    "        res = metric.compute(predictions=true_preds, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": res[\"overall_precision\"],\n",
    "            \"recall\": res[\"overall_recall\"],\n",
    "            \"f1\": res[\"overall_f1\"],\n",
    "            \"accuracy\": res[\"overall_accuracy\"]\n",
    "        }\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "# 种子\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# ------------main----------------\n",
    "def main():\n",
    "    config = Config()\n",
    "    set_seed(config.random_state)\n",
    "\n",
    "    print(\"🚀 CUDA Available:\", torch.cuda.is_available())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---------- 1. 加载数据 ----------\n",
    "    print(\"📥 读取语料...\")\n",
    "    all_data = []\n",
    "    for filename, tag in config.data_files:\n",
    "        data = load_data_with_tag(filename, tag)\n",
    "        all_data.extend(data)\n",
    "    labels_for_split = [d[\"source_tag\"] for d in all_data]\n",
    "    print(f\"📊 总样本数: {len(all_data)}\")\n",
    "\n",
    "    # ---------- 2. 初始化分词器 & 标签映射 ----------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    label2id = config.label2id\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    # ---------- 3. 构建 tokenize 函数 ----------\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_and_align_labels(examples, tokenizer, label2id)\n",
    "\n",
    "    # ---------- 4. 构建评价函数 ----------\n",
    "    compute_metrics_fn = build_compute_metrics(id2label)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # ---------- 5. K 折交叉验证 ----------\n",
    "    kf = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.random_state)\n",
    "    all_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_data, labels_for_split)):\n",
    "        print(f\"\\n{'='*20} Fold {fold+1} {'='*20}\")\n",
    "\n",
    "        # 创建带时间戳的输出目录\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"{config.output_dir_base}{fold+1}_{timestamp}\"\n",
    "\n",
    "        train_data = [all_data[i] for i in train_idx]\n",
    "        val_data = [all_data[i] for i in val_idx]\n",
    "\n",
    "        train_ds = Dataset.from_list(train_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "        val_ds = Dataset.from_list(val_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            config.model_name,\n",
    "            num_labels=len(label2id),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            hidden_dropout_prob=config.dropout_prob,\n",
    "        )\n",
    "        # ❗ 不要手动 .to(device) — Trainer 会自动处理\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.batch_size,\n",
    "            per_device_eval_batch_size=config.batch_size // 2,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=config.eval_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=config.eval_steps,\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            fp16=config.fp16 and torch.cuda.is_available(),\n",
    "            dataloader_num_workers=2,\n",
    "            report_to=\"none\",\n",
    "            save_total_limit=config.save_total_limit,  # 只保留 best\n",
    "            seed=config.random_state,\n",
    "        )\n",
    "\n",
    "        trainer = WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics_fn,\n",
    "            class_weights=config.class_weights,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)],\n",
    "        )\n",
    "\n",
    "        print(\"⏳ 开始训练...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # 🧹 清理中间 checkpoint，只保留 best\n",
    "        cleanup_checkpoints(output_dir)\n",
    "\n",
    "        # 💾 保存最终模型和分词器\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # 📈 评估\n",
    "        eval_res = trainer.evaluate()\n",
    "        print(\"✅ 评估结果:\", eval_res)\n",
    "        all_results.append(eval_res)\n",
    "\n",
    "        # 📂 保存本折结果\n",
    "        with open(f\"{output_dir}/eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(eval_res, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # ---------- 6. 计算平均结果 ----------\n",
    "    avg_precision = np.mean([r[\"eval_precision\"] for r in all_results])\n",
    "    avg_recall = np.mean([r[\"eval_recall\"] for r in all_results])\n",
    "    avg_f1 = np.mean([r[\"eval_f1\"] for r in all_results])\n",
    "\n",
    "    final_results = {\n",
    "        \"avg_precision\": float(avg_precision),\n",
    "        \"avg_recall\": float(avg_recall),\n",
    "        \"avg_f1\": float(avg_f1),\n",
    "        \"fold_results\": all_results\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"🏆 最终平均结果\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"F1:        {avg_f1:.4f}\")\n",
    "\n",
    "    # 💾 保存最终结果\n",
    "    with open(\"final_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n💾 结果已保存至: final_results.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdbacb3-7a4e-4f35-a551-594857923a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CUDA Available: True\n",
      "📥 读取语料...\n",
      "📊 总样本数: 9185\n",
      "🧮 训练集: 7348 | 验证集: 1837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7348/7348 [00:01<00:00, 4523.12 examples/s]\n",
      "Map: 100%|██████████| 1837/1837 [00:00<00:00, 2670.22 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at ku-nlp/deberta-v3-base-japanese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_10116/2350339127.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 96868, 'bos_token_id': 96871, 'pad_token_id': 96869}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ 开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1740/1740 13:14, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.099482</td>\n",
       "      <td>0.799458</td>\n",
       "      <td>0.917574</td>\n",
       "      <td>0.854453</td>\n",
       "      <td>0.967379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.176725</td>\n",
       "      <td>0.869056</td>\n",
       "      <td>0.923795</td>\n",
       "      <td>0.895590</td>\n",
       "      <td>0.976272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.149596</td>\n",
       "      <td>0.863112</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.896036</td>\n",
       "      <td>0.976078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ 删除中间 checkpoint: autodl-tmp/tlv_sqe/result_single_20250917_210317/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/envs/elyza_env/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 最终评估结果:\n",
      "  eval_loss: 0.1496\n",
      "  eval_precision: 0.8631\n",
      "  eval_recall: 0.9316\n",
      "  eval_f1: 0.8960\n",
      "  eval_accuracy: 0.9761\n",
      "  eval_runtime: 5.4234\n",
      "  eval_samples_per_second: 338.7150\n",
      "  eval_steps_per_second: 42.4090\n",
      "  epoch: 30.0000\n",
      "\n",
      "🎉 模型与结果已保存至: ./autodl-tmp/tlv_sqe/result_single_20250917_210317\n"
     ]
    }
   ],
   "source": [
    "#无k-fold\n",
    "# train_single.py\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split  # 👈 改用简单划分\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# ----------------------------\n",
    "# ⚙️ 配置类\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"ku-nlp/deberta-v3-base-japanese\"\n",
    "    data_files: list = None\n",
    "    label2id: dict = None\n",
    "    random_state: int = 42\n",
    "    train_ratio: float = 0.8  # 训练集比例\n",
    "    batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_train_epochs: int = 30  #\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    dropout_prob: float = 0.4\n",
    "    class_weights: list = None\n",
    "    eval_steps: int = 500\n",
    "    early_stopping_patience: int = 20\n",
    "    fp16: bool = True\n",
    "    output_dir: str =  \"./autodl-tmp/tlv_sqe/result_single\"\n",
    "    save_total_limit: int = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data_files is None:\n",
    "            self.data_files = [\n",
    "                (\"data_ketsugou.json\", \"metaphor\"),\n",
    "                (\"data_shihyou.json\", \"simile\"),\n",
    "                (\"data_no.json\", \"non_metaphor\"),\n",
    "            ]\n",
    "        if self.label2id is None:\n",
    "            self.label2id = {\"O\": 0, \"MET\": 1}\n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = [1.0, 30.0]\n",
    "\n",
    "# ----------------------------\n",
    "# 🧠 加权损失 Trainer\n",
    "# ----------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        device = outputs.logits.device\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=-100)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        num_labels = outputs.logits.size(-1)\n",
    "        loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ----------------------------\n",
    "# 🧹 清理中间 checkpoint\n",
    "# ----------------------------\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    output_path = Path(output_dir)\n",
    "    for ckpt_dir in output_path.glob(\"checkpoint-*\"):\n",
    "        print(f\"🗑️ 删除中间 checkpoint: {ckpt_dir}\")\n",
    "        shutil.rmtree(ckpt_dir)\n",
    "\n",
    "# ----------------------------\n",
    "# 📥 加载数据函数\n",
    "# ----------------------------\n",
    "def load_data_with_tag(filename, tag):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        d[\"source_tag\"] = tag\n",
    "    return data\n",
    "\n",
    "# ----------------------------\n",
    "# 🧩 Tokenize + 对齐标签\n",
    "# ----------------------------\n",
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[labels[wid]])\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "# ----------------------------\n",
    "# 📊 评价指标\n",
    "# ----------------------------\n",
    "def build_compute_metrics(id2label):\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_preds = [\n",
    "            [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "            for pred, label in zip(predictions, labels)\n",
    "        ]\n",
    "        res = metric.compute(predictions=true_preds, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": res[\"overall_precision\"],\n",
    "            \"recall\": res[\"overall_recall\"],\n",
    "            \"f1\": res[\"overall_f1\"],\n",
    "            \"accuracy\": res[\"overall_accuracy\"]\n",
    "        }\n",
    "    return compute_metrics\n",
    "\n",
    "# ----------------------------\n",
    "# 🌱 设置随机种子\n",
    "# ----------------------------\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----------------------------\n",
    "# 🚀 主程序（单次训练）\n",
    "# ----------------------------\n",
    "def main():\n",
    "    config = Config()\n",
    "    set_seed(config.random_state)\n",
    "\n",
    "    print(\"🚀 CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "    # ---------- 1. 加载数据 ----------\n",
    "    print(\"📥 读取语料...\")\n",
    "    all_data = []\n",
    "    for filename, tag in config.data_files:\n",
    "        data = load_data_with_tag(filename, tag)\n",
    "        all_data.extend(data)\n",
    "    print(f\"📊 总样本数: {len(all_data)}\")\n",
    "\n",
    "    # ---------- 2. 划分训练集/验证集 ----------\n",
    "    train_data, val_data = train_test_split(\n",
    "        all_data,\n",
    "        train_size=config.train_ratio,\n",
    "        random_state=config.random_state,\n",
    "        shuffle=True,\n",
    "        stratify=[d[\"source_tag\"] for d in all_data]  # 分层抽样，保持比例\n",
    "    )\n",
    "    print(f\"🧮 训练集: {len(train_data)} | 验证集: {len(val_data)}\")\n",
    "\n",
    "    # ---------- 3. 初始化分词器 & 标签映射 ----------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    label2id = config.label2id\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    # ---------- 4. 构建 tokenize 函数 ----------\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_and_align_labels(examples, tokenizer, label2id)\n",
    "\n",
    "    # ---------- 5. 构建评价函数 ----------\n",
    "    compute_metrics_fn = build_compute_metrics(id2label)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # 创建带时间戳的输出目录\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"{config.output_dir}_{timestamp}\"\n",
    "\n",
    "    train_ds = Dataset.from_list(train_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "    val_ds = Dataset.from_list(val_data).map(tokenize_fn, batched=True, batch_size=config.batch_size)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        hidden_dropout_prob=config.dropout_prob,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size // 2,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        weight_decay=config.weight_decay,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config.eval_steps,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=config.fp16 and torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        seed=config.random_state,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        class_weights=config.class_weights,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)],\n",
    "    )\n",
    "\n",
    "    print(\"⏳ 开始训练...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 🧹 清理中间 checkpoint\n",
    "    cleanup_checkpoints(output_dir)\n",
    "\n",
    "    # 💾 保存最终模型\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # 📈 最终评估\n",
    "    eval_res = trainer.evaluate()\n",
    "    print(\"\\n✅ 最终评估结果:\")\n",
    "    for k, v in eval_res.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    # 📂 保存结果\n",
    "    with open(f\"{output_dir}/eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_res, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n🎉 模型与结果已保存至: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65b1fa58-1490-4523-b8c8-43ff23edc8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果：\n",
      "[CLS]\tO\n",
      "何\tO\n",
      "かが\tO\n",
      "燃え\tMET\n",
      "る\tMET\n",
      "[SEP]\tMET\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# ======================\n",
    "# 0. 设置设备\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================\n",
    "# 1. 加载模型和 tokenizer\n",
    "# ======================\n",
    "model_name_or_path = \"./autodl-tmp/tlv_sqe/result_single_20250917_210317\"  # 训练好的模型路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# label 映射\n",
    "id2label = {0: \"O\", 1: \"MET\"}\n",
    "\n",
    "# ======================\n",
    "# 2. 预测函数\n",
    "# ======================\n",
    "def predict_raw_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    输入原始句子（str），返回 List[(token, label)]\n",
    "    \"\"\"\n",
    "    # 使用 tokenizer 自动分词\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False  \n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    # 对齐 subword token\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    result = list(zip(tokens, [id2label[p] for p in predictions]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 测试\n",
    "\n",
    "sentence = \"何かが燃える\"\n",
    "result = predict_raw_sentence(sentence)\n",
    "\n",
    "print(\"预测结果：\")\n",
    "for token, label in result:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bf6971-c13d-4c69-ac61-2a0eeef62d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 sentences...\n",
      "Processed 200 sentences...\n",
      "Processed 300 sentences...\n",
      "Processed 400 sentences...\n",
      "Processed 500 sentences...\n",
      "Processed 600 sentences...\n",
      "Processed 700 sentences...\n",
      "Processed 800 sentences...\n",
      "Processed 900 sentences...\n",
      "Processed 1000 sentences...\n",
      "Processed 1100 sentences...\n",
      "Processed 1200 sentences...\n",
      "Processed 1300 sentences...\n",
      "Processed 1400 sentences...\n",
      "Processed 1500 sentences...\n",
      "Processed 1600 sentences...\n",
      "Processed 1700 sentences...\n",
      "Processed 1800 sentences...\n",
      "Processed 1900 sentences...\n",
      "Processed 2000 sentences...\n",
      "Processed 2100 sentences...\n",
      "Processed 2200 sentences...\n",
      "Processed 2300 sentences...\n",
      "Processed 2400 sentences...\n",
      "Processed 2500 sentences...\n",
      "Processed 2600 sentences...\n",
      "Processed 2700 sentences...\n",
      "Processed 2800 sentences...\n",
      "Processed 2900 sentences...\n",
      "Processed 3000 sentences...\n",
      "Processed 3100 sentences...\n",
      "Processed 3200 sentences...\n",
      "Processed 3300 sentences...\n",
      "Processed 3400 sentences...\n",
      "Processed 3500 sentences...\n",
      "Processed 3600 sentences...\n",
      "Processed 3700 sentences...\n",
      "Processed 3800 sentences...\n",
      "Processed 3900 sentences...\n",
      "Processed 4000 sentences...\n",
      "Processed 4100 sentences...\n",
      "Processed 4200 sentences...\n",
      "Processed 4300 sentences...\n",
      "Processed 4400 sentences...\n",
      "Processed 4500 sentences...\n",
      "Processed 4600 sentences...\n",
      "Processed 4700 sentences...\n",
      "Processed 4800 sentences...\n",
      "Processed 4900 sentences...\n",
      "Processed 5000 sentences...\n",
      "Processed 5100 sentences...\n",
      "✅ Processing complete. Results saved to met_tokens_output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# ======================\n",
    "# 0. 设置设备\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================\n",
    "# 1. 加载模型和 tokenizer\n",
    "# ======================\n",
    "model_name_or_path = \"./autodl-tmp/tlv_sqe/result_single_20250917_210317\"  # 训练好的模型路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# label 映射\n",
    "id2label = {0: \"O\", 1: \"MET\"}\n",
    "\n",
    "# ======================\n",
    "# 2. 预测函数（返回 token + label 列表）\n",
    "# ======================\n",
    "def predict_raw_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    输入原始句子（str），返回 List[(token, label)]\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    result = list(zip(tokens, [id2label.get(p, \"O\") for p in predictions]))\n",
    "    return result\n",
    "\n",
    "# ======================\n",
    "# 3. 批量处理函数\n",
    "# ======================\n",
    "def process_txt_file(input_txt_path: str, output_csv_path: str):\n",
    "    \"\"\"\n",
    "    读取 txt 文件（每行一句），预测每句中的 MET token，\n",
    "    并将结果写入 CSV 文件。\n",
    "    \"\"\"\n",
    "    with open(input_txt_path, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_csv_path, 'w', newline='', encoding='utf-8') as f_out:\n",
    "\n",
    "        writer = csv.writer(f_out)\n",
    "        # 写入表头\n",
    "        writer.writerow([\"sentence\", \"token\", \"position_in_sentence\"])\n",
    "\n",
    "        line_num = 0\n",
    "        for line in f_in:\n",
    "            line_num += 1\n",
    "            sentence = line.strip()\n",
    "            if not sentence:\n",
    "                continue  # 跳过空行\n",
    "\n",
    "            try:\n",
    "                predictions = predict_raw_sentence(sentence)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_num}: {sentence} | {e}\")\n",
    "                continue\n",
    "\n",
    "            # 遍历预测结果，找出 MET 标签的 token\n",
    "            for idx, (token, label) in enumerate(predictions):\n",
    "                if label == \"MET\":\n",
    "                    # 过滤掉特殊 token 如 [CLS], [SEP], [PAD] 等\n",
    "                    if token in tokenizer.all_special_tokens:\n",
    "                        continue\n",
    "                    # 可选：还原 ## 开头的 subword（如 \"##ing\"）\n",
    "                    if token.startswith(\"##\") and idx > 0:\n",
    "                        # 可以选择合并，但为了简单，这里只记录原始 token\n",
    "                        pass\n",
    "                    writer.writerow([sentence, token, idx])\n",
    "\n",
    "            # 可选：打印进度\n",
    "            if line_num % 100 == 0:\n",
    "                print(f\"Processed {line_num} sentences...\")\n",
    "\n",
    "    print(f\"✅ Processing complete. Results saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4. 主程序入口\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_TXT = \"abe_speech_5000.txt\"   # 替换为你的输入文件路径\n",
    "    OUTPUT_CSV = \"met_tokens_output.csv\"\n",
    "\n",
    "    process_txt_file(INPUT_TXT, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c37c0-ae5f-4256-8a7a-a22ee30d6ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elyza",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
