{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonhwe-yang/Japanese-metaphor-detection/blob/main/plugins/hanlp_demo/hanlp_demo/zh/pos_mtl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfGpInivS0fG"
      },
      "source": [
        "## 安装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYwV-UkNNzFp"
      },
      "source": [
        "无论是Windows、Linux还是macOS，HanLP的安装只需一句话搞定："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Uf_u7ddMhUt",
        "outputId": "4c4d69e9-574a-49ad-b864-72295d8a0f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hanlp\n",
            "  Downloading hanlp-2.1.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hanlp-common>=0.0.23 (from hanlp)\n",
            "  Downloading hanlp_common-0.0.23.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-downloader (from hanlp)\n",
            "  Downloading hanlp_downloader-0.0.25.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hanlp-trie>=0.0.4 (from hanlp)\n",
            "  Downloading hanlp_trie-0.0.5.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (from hanlp) (12.0.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from hanlp) (0.2.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from hanlp) (3.1.0)\n",
            "Collecting toposort==1.5 (from hanlp)\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from hanlp) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from hanlp) (4.57.1)\n",
            "Collecting phrasetree>=0.0.9 (from hanlp-common>=0.0.23->hanlp)\n",
            "  Downloading phrasetree-0.0.9.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->hanlp) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.1.1->hanlp) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml->hanlp) (12.575.51)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.1.1->hanlp) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->hanlp) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->hanlp) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.1.1->hanlp) (2025.10.5)\n",
            "Downloading hanlp-2.1.3-py3-none-any.whl (654 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m654.1/654.1 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: hanlp-common, hanlp-trie, hanlp-downloader, phrasetree\n",
            "  Building wheel for hanlp-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-common: filename=hanlp_common-0.0.23-py3-none-any.whl size=30819 sha256=d050d35caf7785526105db772f5d0b4ce8ccc7605bebf3529f207cb71ba8eb86\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/22/2d/75d505250ca2b83d52998e97db4d3ed89315138752d2e22794\n",
            "  Building wheel for hanlp-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-trie: filename=hanlp_trie-0.0.5-py3-none-any.whl size=6816 sha256=6f252a66a22650be009a0d1541c7c50c8d92e7718f2a9e22b3f791f9fc1d5e5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/a2/e2/b2a458cadfb9ff912848b0e6205bfa5ac628daa5156ccac651\n",
            "  Building wheel for hanlp-downloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanlp-downloader: filename=hanlp_downloader-0.0.25-py3-none-any.whl size=13746 sha256=6fe384fa78b82b1a57d5c9a7b87715c048aa9b6b1a8728395b903505fc28b0a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/59/2c/1373729f96160c9cded2968f6c33377aab5c95ba1002b559fd\n",
            "  Building wheel for phrasetree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phrasetree: filename=phrasetree-0.0.9-py3-none-any.whl size=44218 sha256=43f07d0432ea337672e9a20e2921ce1be4afdb348e76582babbd976502e8f64f\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/6c/77/51fc144dbf90ff2b9e898ce24082958e63388341de3567ace3\n",
            "Successfully built hanlp-common hanlp-trie hanlp-downloader phrasetree\n",
            "Installing collected packages: toposort, phrasetree, hanlp-common, hanlp-trie, hanlp-downloader, hanlp\n",
            "Successfully installed hanlp-2.1.3 hanlp-common-0.0.23 hanlp-downloader-0.0.25 hanlp-trie-0.0.5 phrasetree-0.0.9 toposort-1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install hanlp -U\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-1KqEOOJ4t"
      },
      "source": [
        "## 加载模型\n",
        "HanLP的工作流程是先加载模型，模型的标示符存储在`hanlp.pretrained`这个包中，按照NLP任务归类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M7ka0K5OMWU",
        "outputId": "2972b749-dc8f-4b17-cdec-c25200a94984"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small_20201223_035557.zip',\n",
              " 'OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base_20201223_201906.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20220626_175100.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip',\n",
              " 'CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH': 'https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_ernie_gram_base_aug_20210904_145403.zip',\n",
              " 'KYOTO_EVAHAN_TOK_LEM_POS_UDEP_LZH': 'https://file.hankcs.com/hanlp/mtl/kyoto_evahan_tok_lem_pos_udep_bert-ancient-chinese_lr_1_aug_dict_20250112_154422.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L6_no_space_20220731_161526.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12_no_space_20220807_133143.zip',\n",
              " 'UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE': 'https://file.hankcs.com/hanlp/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_xlm_base_20220608_003435.zip',\n",
              " 'NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA': 'https://file.hankcs.com/hanlp/mtl/npcmj_ud_kyoto_tok_pos_ner_dep_con_srl_bert_base_char_ja_20210914_133742.zip',\n",
              " 'EN_TOK_LEM_POS_NER_SRL_UDEP_SDP_CON_MODERNBERT_BASE': 'https://file.hankcs.com/hanlp/mtl/en_tok_lem_pos_ner_srl_udep_sdp_con_modernbert_base_prepend_false_20241229_053838.zip',\n",
              " 'EN_TOK_LEM_POS_NER_SRL_UDEP_SDP_CON_MODERNBERT_LARGE': 'https://file.hankcs.com/hanlp/mtl/en_tok_lem_pos_ner_srl_udep_sdp_con_modernbert_large_prepend_false_20250107_181612.zip'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import hanlp\n",
        "hanlp.pretrained.mtl.ALL # MTL多任务，具体任务见模型名称，语种见名称最后一个字段或相应语料库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMW528wGNulM"
      },
      "source": [
        "调用`hanlp.load`进行加载，模型会自动下载到本地缓存。自然语言处理分为许多任务，分词只是最初级的一个。与其每个任务单独创建一个模型，不如利用HanLP的联合模型一次性完成多个任务："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0tmKBu7sNAXX",
        "outputId": "2b29d76e-bbd8-4b88-f638-637db6b1c29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip\n",
            "100% 467.9 MiB 716.3 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip to /root/.hanlp/mtl\n",
            "Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip\n",
            "100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/transformers/electra_zh_base_20210706_125233.zip to /root/.hanlp/transformers\n",
            "Downloading https://file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip\n",
            "100%  19.4 KiB   4.9 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing /root/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.json.zip to /root/.hanlp/thirdparty/file.hankcs.com/corpus\n"
          ]
        }
      ],
      "source": [
        "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elA_UyssOut_"
      },
      "source": [
        "## 词性标注\n",
        "任务越少，速度越快。如指定仅执行词性标注，默认CTB标准："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "BqEmDMGGOtk3",
        "outputId": "ed30eb6d-5ca4-40cf-f351-e6798914c345"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">香港/NR&nbsp;记者/NN&nbsp;采访/VV&nbsp;时/LC&nbsp;，/PU&nbsp;张狂/VA&nbsp;地/DEV&nbsp;宣扬/VV&nbsp;其/PN&nbsp;“/PU&nbsp;台独/NN&nbsp;”/PU&nbsp;主张/NN&nbsp;，/PU&nbsp;引起/VV&nbsp;香港/NR&nbsp;各界/NN&nbsp;人士/NN&nbsp;的/DEG&nbsp;[/PU&nbsp;愤怒/NN&nbsp;]/PU&nbsp;。/PU&nbsp;他们/PN&nbsp;群起/AD&nbsp;抨击/VV&nbsp;吕秀莲/NR&nbsp;的/DEG&nbsp;荒谬/JJ&nbsp;主张/NN&nbsp;，/PU&nbsp;指出/VV&nbsp;她/PN&nbsp;的/DEG&nbsp;这/DT&nbsp;番/M&nbsp;言论/NN&nbsp;，/PU&nbsp;是/VC&nbsp;背弃/VV&nbsp;自己/PN&nbsp;祖.../PU</pre></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "HanLP(['香港记者采访时，张狂地宣扬其“台独”主张，引起香港各界人士的[愤怒]。他们群起抨击吕秀莲的荒谬主张，指出她的这番言论，是背弃自己祖...'], tasks='pos/ctb').pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj1Jk-2sPHYx"
      },
      "source": [
        "注意上面两个“希望”的词性各不相同，一个是名词另一个是动词。\n",
        "执行PKU词性标注："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxctCigrTKu-"
      },
      "source": [
        "同时执行所有标准的词性标注："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo08uquCTFSk",
        "outputId": "d2b3eb65-06e6-47a6-d954-04cae27d6c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"tok/fine\": [\n",
            "    [\"HanLP\", \"为\", \"生产\", \"环境\", \"带来\", \"次\", \"世代\", \"最\", \"先进\", \"的\", \"多语种\", \"NLP\", \"技术\", \"。\"],\n",
            "    [\"我\", \"的\", \"希望\", \"是\", \"希望\", \"张晚霞\", \"的\", \"背影\", \"被\", \"晚霞\", \"映红\", \"。\"]\n",
            "  ],\n",
            "  \"pos/ctb\": [\n",
            "    [\"NR\", \"P\", \"NN\", \"NN\", \"VV\", \"JJ\", \"NN\", \"AD\", \"JJ\", \"DEG\", \"NN\", \"NR\", \"NN\", \"PU\"],\n",
            "    [\"PN\", \"DEG\", \"NN\", \"VC\", \"VV\", \"NR\", \"DEG\", \"NN\", \"LB\", \"NN\", \"VV\", \"PU\"]\n",
            "  ],\n",
            "  \"pos/pku\": [\n",
            "    [\"nx\", \"p\", \"vn\", \"n\", \"v\", \"b\", \"n\", \"d\", \"a\", \"u\", \"n\", \"nx\", \"n\", \"w\"],\n",
            "    [\"r\", \"u\", \"n\", \"v\", \"v\", \"nr\", \"u\", \"n\", \"p\", \"n\", \"v\", \"w\"]\n",
            "  ],\n",
            "  \"pos/863\": [\n",
            "    [\"w\", \"p\", \"v\", \"n\", \"v\", \"a\", \"nt\", \"d\", \"a\", \"u\", \"n\", \"ws\", \"n\", \"w\"],\n",
            "    [\"r\", \"u\", \"n\", \"vl\", \"v\", \"nh\", \"u\", \"n\", \"p\", \"n\", \"v\", \"w\"]\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(HanLP(['HanLP为生产环境带来次世代最先进的多语种NLP技术。', '我的希望是希望张晚霞的背影被晚霞映红。'], tasks='pos*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2fg2Dab6Ppd"
      },
      "source": [
        "以`pos`开头的字段为词性，以`tok`开头的第一个数组为单词，两者按下标一一对应。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xr71Rymg_zEL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HanLP = hanlp.load(\n",
        "    hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "def extract_fennu_sentence(text):\n",
        "    pattern = r'[^。！？，]*\\[愤怒\\][^。！？，]*[。！？，]?'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return match.group(0).replace('[', '').replace(']', '').strip()\n",
        "    return None\n",
        "\n",
        "def is_fennu_noun(sentence):\n",
        "    try:\n",
        "        doc = HanLP(sentence, tasks='pos')  # 你想的原样调用\n",
        "        tokens = doc['tok/fine']\n",
        "        pos_tags = doc['pos/ctb']\n",
        "        return any(w == \"愤怒\" and p.startswith(\"n\") for w, p in zip(tokens, pos_tags))\n",
        "    except Exception as e:\n",
        "        print(f\"调用失败: {e}\")\n",
        "        return False\n",
        "\n",
        "def filter_excel_fennu_nouns(input_file, output_file, column_name=\"内容\"):\n",
        "    df = pd.read_excel(input_file)\n",
        "    selected_rows = []\n",
        "\n",
        "    for idx, text in tqdm(df[column_name].astype(str).items(), desc=\"Processing rows\"):\n",
        "        sentence = extract_fennu_sentence(text)\n",
        "        if sentence and is_fennu_noun(sentence):\n",
        "            selected_rows.append(df.iloc[idx])\n",
        "\n",
        "    if selected_rows:\n",
        "        output_df = pd.DataFrame(selected_rows)\n",
        "        output_df.to_excel(output_file, index=False)\n",
        "        print(f\"筛选完成，结果已保存到 {output_file}\")\n",
        "    else:\n",
        "        print(\"没有符合条件的行\")\n",
        "\n",
        "# -------------------- 使用示例 --------------------\n",
        "input_file = \"/content/愤怒CCL1990-2010.xlsx\"\n",
        "output_file = \"/content/愤怒CCL1990-2010筛选.xlsx\"\n",
        "\n",
        "filter_excel_fennu_nouns(input_file, output_file, column_name=\"内容\")"
      ],
      "metadata": {
        "id": "ffum6TTG_fg-",
        "outputId": "333a5079-ffe6-4095-d00b-ba838bfbaf08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 20001it [04:14, 78.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "没有符合条件的行\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqeUvD3p6Ppd"
      },
      "source": [
        "#### 注意\n",
        "Native API的输入单位限定为句子，需使用[多语种分句模型](https://github.com/hankcs/HanLP/blob/master/plugins/hanlp_demo/hanlp_demo/sent_split.py)或[基于规则的分句函数](https://github.com/hankcs/HanLP/blob/master/hanlp/utils/rules.py#L19)先行分句。RESTful同时支持全文、句子、已分词的句子。除此之外，RESTful和native两种API的语义设计完全一致，用户可以无缝互换。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q4MUpgVQNlu"
      },
      "source": [
        "自定义单个词性："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-9gAeIVQUFG"
      },
      "source": [
        "根据上下文自定义词性："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR0AEzsY6Ppe"
      },
      "source": [
        "需要算法基础才能理解，初学者可参考[《自然语言处理入门》](http://nlp.hankcs.com/book.php)。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pos_mtl.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}